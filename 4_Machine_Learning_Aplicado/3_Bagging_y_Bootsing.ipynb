{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39162410",
   "metadata": {},
   "source": [
    "# Bagging y Boosting\n",
    "\n",
    "## Bagging (Boostrap Aggregating)\n",
    "- Entrenar muchos modelos débiles (ej. árboles de decisión) sobre muestras aleatorias con reemplazo del dataset.\n",
    "- Cada modelo vota (clasificación) o promedia (regresión).\n",
    "- Reduce varianza -> mejora estabilidad.\n",
    "- Ejemplo: Random Forest (es un bagging + selección aleatoria de features).\n",
    "- Ejemplo real: Es como preguntar la misma pregunta a un grupo de personas que vieron cosas distintas -> se reduce la posibilidad de error de unos solo.\n",
    "\n",
    "## Boosting\n",
    "- Entrenar modelos secuenciales: cada modelo aprende de los errores del anterior.\n",
    "- Los errores se pesan más fuerte en el siguiente modelo.\n",
    "- La predicción final es una combinación ponderada de todos los modelos.\n",
    "- Reduce el sesgo -> mejora precisión.\n",
    "- Ejemplo real: Es como un profesor que revisa un examen varias veces, y cada vez se concentra más en las preguntas donde el alumno falló.\n",
    "\n",
    "## Diferencias Bagging vs Boosting\n",
    "| Aspecto | Bagging | Boosting |\n",
    "|---------|---------|----------|\n",
    "| Entrenamiento | En paralelo (cada modelo independiente) | Secuencial ( cada modelo depende de los errores previos) |\n",
    "| Objetivo | Reducir varianza | Reducir sesgo |\n",
    "| ejemplo | Random Forest | AdaBoost, Gradient Boosting, XGBoost, Light GBM |\n",
    "| Riesgo | Puede generar modelos muy grandes | Riesgo de overfitting si no se controla |\n",
    "\n",
    "## Ejemplo - AdaBoost con Dataset Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8793a539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporte de clasificación:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        15\n",
      "  versicolor       0.88      0.93      0.90        15\n",
      "   virginica       0.93      0.87      0.90        15\n",
      "\n",
      "    accuracy                           0.93        45\n",
      "   macro avg       0.93      0.93      0.93        45\n",
      "weighted avg       0.93      0.93      0.93        45\n",
      "\n",
      "Matriz de confusión:\n",
      " [[15  0  0]\n",
      " [ 0 14  1]\n",
      " [ 0  2 13]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 1. Dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "target_names = iris.target_names\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 2. AdaBoost\n",
    "clf = AdaBoostClassifier(\n",
    "    n_estimators=50,       # número de clasificadores débiles\n",
    "    learning_rate=1.0,     # peso de cada modelo secuencial\n",
    "    random_state=42\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 3. Evaluación\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Reporte de clasificación:\\n\", classification_report(y_test, y_pred, target_names=target_names))\n",
    "print(\"Matriz de confusión:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48422148",
   "metadata": {},
   "source": [
    "## Parámetros clave de AdaBoost\n",
    "- `n_estimators`: cuántos modelos secuenciales se entrenan.\n",
    "- `learning_rate`: controla cuánto peso damos a cada nuevo modelo (trade-off entre muchos modelos pequeños o pocos más fuertes).\n",
    "- `base_estimator`: por defecto usa DecisionTreeClassifier(max_depth=1) (un \"stump\" o árbol mas pequeño).\n",
    "\n",
    "\n",
    "## Explicación\n",
    "- AdaBoost combina muchos árboles débiles (stumps) para formar un clasificador fuerte.\n",
    "- Cada nuevo árbol se enfoca en las muestras que el anterior clasificó mal.\n",
    "- La predicción final es un promedio ponderado de todos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b51ebb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
